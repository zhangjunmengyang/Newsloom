"""Layer 2.5: æ¨èç³»ç»Ÿå¼æ’åº â€” ç²—æ’ + ç²¾æ’ + å»é‡èšåˆ

è®¾è®¡æ€æƒ³ï¼š
- å€™é€‰æ± ï¼šæ‰€æœ‰æº fetch åçš„å…¨é‡æ•°æ®
- ç²—æ’ï¼ˆè§„åˆ™ï¼‰ï¼šå…³é”®è¯ Ã— æ¥æºæƒå¨åº¦ Ã— æ—¶æ•ˆæ€§ Ã— äº’åŠ¨é‡ â†’ å¿«é€Ÿæ‰“åˆ†ï¼Œå– Top N
- ç²¾æ’ï¼ˆAIï¼‰ï¼šClaude è¯„ä¼° relevance Ã— impact Ã— urgency â†’ ç²¾ç¡®æ’åº
- å»é‡èšåˆï¼šåŒä¸€äº‹ä»¶å¤šæºæŠ¥é“åˆå¹¶
- ä¸ªæ€§åŒ–ï¼šæ ¹æ®ç”¨æˆ·ç”»åƒè°ƒæ•´æƒé‡
"""

import re
import hashlib
from typing import List, Dict, Tuple, Optional
from datetime import datetime, timezone, timedelta
from collections import defaultdict

from sources.base import Item


# ============================================================
# ç”¨æˆ·ç”»åƒ â€” è€æ¿çš„å…´è¶£æƒé‡
# ============================================================
USER_PROFILE = {
    # AI/ML æ–¹å‘ï¼ˆæ ¸å¿ƒå…´è¶£ï¼‰
    "ai_core": {
        "weight": 2.0,
        "keywords": [
            "llm", "large language model", "transformer", "gpt", "claude",
            "gemini", "reasoning", "agent", "rag", "fine-tuning", "rlhf",
            "multimodal", "diffusion", "openai", "anthropic", "deepseek",
            "mistral", "scaling law", "inference", "quantization", "moe",
            "å¤§æ¨¡å‹", "æ¨ç†", "æ™ºèƒ½ä½“", "å¾®è°ƒ",
        ]
    },
    # AI åº”ç”¨/å·¥ç¨‹ï¼ˆå·¥ä½œç›¸å…³ï¼‰
    "ai_engineering": {
        "weight": 1.8,
        "keywords": [
            "deployment", "serving", "mlops", "vector database", "embedding",
            "prompt engineering", "function calling", "tool use", "ai agent",
            "coding assistant", "copilot", "cursor", "vscode", "ide",
            "api", "sdk", "framework", "benchmark", "evaluation",
        ]
    },
    # Crypto/é‡åŒ–ï¼ˆç¬¬äºŒèµ›é“ï¼‰
    "crypto_quant": {
        "weight": 1.8,
        "keywords": [
            "bitcoin", "ethereum", "btc", "eth", "solana", "defi",
            "trading", "quantitative", "algorithmic", "arbitrage",
            "market making", "liquidity", "on-chain", "whale",
            "polymarket", "prediction market", "perpetual", "futures",
            "stablecoin", "usdc", "usdt", "layer 2", "rollup",
            "æ¯”ç‰¹å¸", "ä»¥å¤ªåŠ", "é‡åŒ–", "å¥—åˆ©", "é“¾ä¸Š",
        ]
    },
    # å¼€æº/å·¥å…·é“¾
    "tools": {
        "weight": 1.3,
        "keywords": [
            "open source", "github", "rust", "python", "typescript",
            "cli", "terminal", "developer tool", "productivity",
            "automation", "self-hosted", "homelab",
        ]
    },
    # åˆ›ä¸š/å•†ä¸š
    "business": {
        "weight": 1.0,
        "keywords": [
            "startup", "funding", "acquisition", "ipo", "revenue",
            "valuation", "series a", "series b", "unicorn",
        ]
    },
}

# æ¥æºæƒå¨åº¦åˆ†æ•°ï¼ˆæ»¡åˆ† 10ï¼‰
SOURCE_AUTHORITY = {
    # Tier 1: å®˜æ–¹/é¡¶çº§
    "OpenAI Blog": 10, "Google AI Blog": 10, "Google DeepMind": 10,
    "Anthropic": 10, "Meta Engineering": 9, "Microsoft Research": 9,
    "HuggingFace Blog": 9, "NVIDIA AI": 9,
    # Tier 2: é¡¶çº§åª’ä½“
    "MIT Tech Review": 9, "arXiv": 9, "The Gradient": 8,
    "Lil'Log (Lilian Weng)": 9, "Simon Willison": 8,
    "Import AI (Jack Clark)": 8,
    # Tier 3: ä¸»æµç§‘æŠ€åª’ä½“
    "TechCrunch": 7, "The Verge": 7, "Ars Technica": 7,
    "Wired": 7, "VentureBeat AI": 7, "The Decoder": 7,
    "InfoQ": 7, "Lobsters": 7,
    # Tier 4: è¡Œä¸šåª’ä½“
    "CoinDesk": 7, "Cointelegraph": 6, "CryptoSlate": 6,
    "Decrypt": 6, "Bankless": 7, "The Defiant": 6,
    "WSJ Markets": 8, "Financial Times": 8, "CNBC": 7,
    # Tier 5: ç¤¾åŒº/ç»¼åˆ
    "Hacker News": 7, "GitHub Trending": 7,
    "AI News": 6, "36kr": 6, "æœºå™¨ä¹‹å¿ƒ": 7, "é‡å­ä½": 6,
    # Tier 6: ä¸€èˆ¬
    "Dev.to AI": 5, "Slashdot": 5,
    # äº¤æ˜“æ‰€ä¸Šçº¿ (é«˜ä»·å€¼ alpha ä¿¡å·)
    "exchange_listing": 10,
    "Binance": 10, "Upbit": 10, "Bithumb": 10,
    "Coinbase": 9, "OKX": 9, "Bybit": 8,
    # Anthropic å®˜æ–¹
    "anthropic_news": 10, "Anthropic News": 10,
    # Web Search
    "web_search": 6,
    # é»˜è®¤
    "_default": 5,
}


class CoarseRanker:
    """
    ç²—æ’å™¨ â€” è§„åˆ™æ‰“åˆ†ï¼Œå¿«é€Ÿç­›é€‰å€™é€‰æ± 
    
    è¯„åˆ†ç»´åº¦ï¼š
    1. å…³é”®è¯åŒ¹é…ï¼ˆä¸ç”¨æˆ·ç”»åƒå¯¹é½ï¼‰
    2. æ¥æºæƒå¨åº¦
    3. æ—¶æ•ˆæ€§è¡°å‡
    4. äº’åŠ¨é‡åŠ æˆï¼ˆHN scoreã€GitHub stars ç­‰ï¼‰
    """

    def __init__(self, user_profile: dict = None, source_authority: dict = None):
        self.profile = user_profile or USER_PROFILE
        self.authority = source_authority or SOURCE_AUTHORITY
        # é¢„ç¼–è¯‘å…³é”®è¯æ­£åˆ™
        self._keyword_patterns = {}
        for category, info in self.profile.items():
            patterns = []
            for kw in info["keywords"]:
                patterns.append(re.compile(re.escape(kw), re.IGNORECASE))
            self._keyword_patterns[category] = (patterns, info["weight"])

    def rank(self, items: List[Item], top_n: int = 200) -> List[Item]:
        """
        ç²—æ’ï¼šå¯¹æ‰€æœ‰å€™é€‰æ‰“åˆ†ï¼Œè¿”å› Top N
        
        Args:
            items: å…¨é‡å€™é€‰
            top_n: è¿”å›å‰ N æ¡
        Returns:
            æŒ‰åˆ†æ•°é™åºæ’åˆ—çš„ Top N items
        """
        scored = []
        for item in items:
            score = self._score_item(item)
            item.score = score
            scored.append((score, item))

        # æŒ‰åˆ†æ•°é™åº
        scored.sort(key=lambda x: x[0], reverse=True)

        result = [item for _, item in scored[:top_n]]
        return result

    def _score_item(self, item: Item) -> float:
        """
        ç»¼åˆæ‰“åˆ†
        
        score = keyword_score Ã— authority Ã— freshness Ã— engagement_bonus
        """
        text = f"{item.title} {item.text}".lower()

        # 1. å…³é”®è¯å¾—åˆ†ï¼ˆç”¨æˆ·ç”»åƒåŒ¹é…ï¼‰
        kw_score = 0.0
        for category, (patterns, weight) in self._keyword_patterns.items():
            match_count = 0
            for pat in patterns:
                if pat.search(text):
                    match_count += 1
            if match_count > 0:
                # æ¯ä¸ª category çš„è´¡çŒ® = min(matches, 3) Ã— weight
                kw_score += min(match_count, 3) * weight

        # åŸºç¡€åˆ†ï¼šå³ä½¿æ²¡åŒ¹é…å…³é”®è¯ä¹Ÿæœ‰åŸºç¡€åˆ†ï¼ˆé˜²æ­¢å¥½å†…å®¹è¢«åŸ‹ï¼‰
        base_score = max(kw_score, 1.0)

        # 2. æ¥æºæƒå¨åº¦ (0.5 ~ 1.0)
        meta = item.metadata or {}
        source_name = meta.get("feed_name") or meta.get("feed_title") or item.source
        authority = self.authority.get(source_name, self.authority["_default"])
        authority_factor = 0.5 + (authority / 20.0)  # 5 â†’ 0.75, 10 â†’ 1.0

        # 3. æ—¶æ•ˆæ€§è¡°å‡ (1.0 â†’ 0.3 over 48h) + breaking news å‡æƒ
        now = datetime.now(timezone.utc)
        age_hours = (now - item.published_at).total_seconds() / 3600
        freshness = max(0.3, 1.0 - (age_hours / 72.0))
        # Breaking news (<2h) é¢å¤– +30%
        if age_hours < 2:
            freshness = min(freshness * 1.3, 1.5)

        # 4. äº’åŠ¨é‡åŠ æˆ
        engagement = 1.0
        if meta.get("score"):  # HN score
            hn_score = meta["score"]
            engagement += min(hn_score / 500, 1.0)  # 500+ score â†’ +1.0
        if meta.get("stars"):  # GitHub stars
            stars = meta["stars"]
            engagement += min(stars / 10000, 0.8)
        if meta.get("daily_stars"):
            daily = meta["daily_stars"]
            engagement += min(daily / 500, 0.5)
        if meta.get("comments"):
            comments = meta["comments"]
            engagement += min(comments / 200, 0.3)

        # 5. ç‰¹æ®Šä¿¡å·åŠ æˆ
        # äº¤æ˜“æ‰€ä¸Šçº¿ â†’ å¼ºåˆ¶é«˜åˆ†ï¼ˆè¿™æ˜¯ alpha ä¿¡å·ï¼Œä¸èƒ½è¢«åŸ‹ï¼‰
        if item.source == 'exchange_listing':
            title_lower = item.title.lower()
            # çœŸå®ä¸Šçº¿å…¬å‘Šï¼ˆæ’é™¤ CoinGecko Trending è¿™ç±»ï¼‰
            if any(k in title_lower for k in ['ä¸Šçº¿', 'listing', 'new pair', 'new trading', 'æ–°å¢']):
                base_score = max(base_score, 20.0)
            # éŸ©å›½äº¤æ˜“æ‰€é¢å¤–åŠ åˆ†ï¼ˆæº¢ä»·æ•ˆåº”ï¼‰
            if 'ğŸ‡°ğŸ‡·' in item.title or any(k in title_lower for k in ['upbit', 'bithumb']):
                base_score *= 1.5

        # Anthropic å®˜æ–¹å…¬å‘Š â†’ å›ºå®šé«˜åˆ†
        if item.source in ('anthropic_news', 'anthropic'):
            base_score = max(base_score, 12.0)

        final_score = base_score * authority_factor * freshness * engagement
        return round(final_score, 3)


class Deduplicator:
    """
    å»é‡èšåˆå™¨ â€” åŒä¸€äº‹ä»¶å¤šæºæŠ¥é“åˆå¹¶
    
    ç­–ç•¥ï¼š
    1. URL å»é‡ï¼ˆç²¾ç¡®åŒ¹é…ï¼‰
    2. æ ‡é¢˜ç›¸ä¼¼åº¦ï¼ˆç¼–è¾‘è·ç¦» / Jaccardï¼‰
    3. ä¿ç•™æœ€é«˜åˆ†çš„ç‰ˆæœ¬ï¼Œå…¶ä½™åˆå¹¶ä¸º related_sources
    """

    def __init__(self, similarity_threshold: float = 0.6):
        self.threshold = similarity_threshold

    def deduplicate(self, items: List[Item]) -> List[Item]:
        """
        å»é‡å¹¶èšåˆ
        
        Returns:
            å»é‡åçš„ itemsï¼ˆå·²æ’åºï¼‰
        """
        if not items:
            return []

        # Phase 1: URL å»é‡
        seen_urls = {}
        url_deduped = []
        for item in items:
            normalized_url = self._normalize_url(item.url)
            if normalized_url in seen_urls:
                # ä¿ç•™åˆ†æ•°æ›´é«˜çš„
                existing = seen_urls[normalized_url]
                if item.score > existing.score:
                    url_deduped.remove(existing)
                    url_deduped.append(item)
                    seen_urls[normalized_url] = item
            else:
                seen_urls[normalized_url] = item
                url_deduped.append(item)

        # Phase 2: æ ‡é¢˜ç›¸ä¼¼åº¦å»é‡
        clusters = self._cluster_by_title(url_deduped)

        # æ¯ä¸ª cluster å–æœ€é«˜åˆ†
        result = []
        for cluster in clusters:
            best = max(cluster, key=lambda x: x.score)
            if len(cluster) > 1:
                # è®°å½•å…¶ä»–æ¥æº
                other_sources = [
                    (item.metadata or {}).get("feed_name", item.source)
                    for item in cluster if item != best
                ]
                best.metadata = best.metadata or {}
                best.metadata["related_sources"] = other_sources
                best.metadata["coverage_count"] = len(cluster)
            result.append(best)

        return result

    def _normalize_url(self, url: str) -> str:
        """URL æ ‡å‡†åŒ–"""
        url = url.strip().rstrip("/")
        # å»æ‰å¸¸è§è¿½è¸ªå‚æ•°
        for param in ["utm_source", "utm_medium", "utm_campaign", "ref"]:
            url = re.sub(rf'[?&]{param}=[^&]*', '', url)
        return url

    def _cluster_by_title(self, items: List[Item]) -> List[List[Item]]:
        """æŒ‰æ ‡é¢˜ç›¸ä¼¼åº¦èšç±»"""
        clusters = []
        used = set()

        for i, item_a in enumerate(items):
            if i in used:
                continue
            cluster = [item_a]
            used.add(i)

            tokens_a = self._tokenize(item_a.title)

            for j, item_b in enumerate(items):
                if j in used or j <= i:
                    continue
                tokens_b = self._tokenize(item_b.title)
                sim = self._jaccard(tokens_a, tokens_b)
                if sim >= self.threshold:
                    cluster.append(item_b)
                    used.add(j)

            clusters.append(cluster)

        return clusters

    def _tokenize(self, text: str) -> set:
        """åˆ†è¯ï¼ˆç®€å•æŒ‰ç©ºæ ¼ + ç‰¹æ®Šå­—ç¬¦ï¼‰"""
        words = re.findall(r'\w+', text.lower())
        # å»åœç”¨è¯
        stop_words = {"the", "a", "an", "is", "are", "was", "were", "in", "on",
                       "at", "to", "for", "of", "and", "or", "with", "from", "by",
                       "that", "this", "it", "its", "how", "what", "why", "when"}
        return {w for w in words if w not in stop_words and len(w) > 1}

    def _jaccard(self, set_a: set, set_b: set) -> float:
        """Jaccard ç›¸ä¼¼åº¦"""
        if not set_a or not set_b:
            return 0.0
        intersection = len(set_a & set_b)
        union = len(set_a | set_b)
        return intersection / union if union > 0 else 0.0


class RankingPipeline:
    """
    å®Œæ•´æ’åº pipeline
    
    å€™é€‰æ±  â†’ ç²—æ’ â†’ å»é‡ â†’ [ç²¾æ’ç”± analyzer è´Ÿè´£]
    """

    def __init__(self, config: dict = None):
        self.config = config or {}
        self.coarse_ranker = CoarseRanker()
        self.deduplicator = Deduplicator(
            similarity_threshold=self.config.get("dedup_threshold", 0.55)
        )

    def process(self, items: List[Item], top_n: int = 200) -> List[Item]:
        """
        æ‰§è¡Œç²—æ’ + å»é‡
        
        Args:
            items: å…¨é‡å€™é€‰
            top_n: ç²—æ’åä¿ç•™æ¡æ•°
        Returns:
            æ’åº+å»é‡åçš„ items
        """
        print(f"\nğŸ“Š æ’åºç®¡çº¿å¯åŠ¨...")
        print(f"   å€™é€‰æ± : {len(items)} æ¡")

        # Step 1: ç²—æ’
        ranked = self.coarse_ranker.rank(items, top_n=top_n)
        print(f"   ç²—æ’ Top {top_n}: {len(ranked)} æ¡")
        if ranked:
            print(f"   åˆ†æ•°èŒƒå›´: {ranked[0].score:.2f} ~ {ranked[-1].score:.2f}")

        # Step 2: å»é‡
        deduped = self.deduplicator.deduplicate(ranked)
        removed = len(ranked) - len(deduped)
        if removed > 0:
            print(f"   å»é‡: ç§»é™¤ {removed} æ¡é‡å¤")

        print(f"   âœ… æœ€ç»ˆ: {len(deduped)} æ¡")
        return deduped
