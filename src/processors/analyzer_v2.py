"""Layer 3 v2: AI ç²¾æ’ + æ´å¯Ÿç”Ÿæˆ + Executive Summary

æ”¹è¿›ç‚¹ï¼š
1. ç²¾æ’ï¼šClaude è¯„ä¼° relevance/impact/urgency ä¸‰ç»´æ‰“åˆ†
2. æ´å¯Ÿï¼šä¸åªæ˜¯æ‘˜è¦ï¼ŒåŒ…å« "so what" + priority + tags
3. Executive Summaryï¼šè·¨æ¿å—çš„ 30s å¿«è¯»
4. Token-aware åˆ†æ‰¹ + å®¹é”™
"""

import json
from typing import List, Dict, Optional
from pathlib import Path
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor, as_completed

from sources.base import Item
from ai.claude import ClaudeClient
from ai.prompts_v2 import PromptsV2


class AIAnalyzerV2:
    """
    AI åˆ†æå™¨ v2 â€” ç²¾æ’ + æ´å¯Ÿ
    
    Pipeline:
    1. æŒ‰ section åˆ†ç»„
    2. æ¯ä¸ª section: ç²¾æ’ï¼ˆç­›é€‰ + æ’åºï¼‰ â†’ æ´å¯Ÿæå–
    3. å…¨å±€: Executive Summary ç”Ÿæˆ
    """

    def __init__(self, claude_client: ClaudeClient, config: dict = None, max_workers: int = 4):
        self.claude = claude_client
        self.config = config or {}
        self.prompts = PromptsV2()
        self.max_workers = max_workers

    def analyze(self, items: List[Item], top_per_section: int = 10) -> Dict:
        """
        å®Œæ•´åˆ†ææµç¨‹
        
        Args:
            items: ç²—æ’åçš„å€™é€‰ï¼ˆå·²æŒ‰ section åˆ†å¥½ or æ··åˆï¼‰
            top_per_section: æ¯ä¸ª section ç²¾æ’åä¿ç•™æ¡æ•°
        
        Returns:
            {
                "briefs": {section: [brief, ...]},
                "executive_summary": "...",
                "stats": {...}
            }
        """
        print(f"\nğŸ§  AI åˆ†æ v2 å¯åŠ¨...")
        print(f"   æ¨¡å‹: {self.claude.model}")

        # æŒ‰ section åˆ†ç»„
        by_section = self._group_by_section(items)

        all_briefs = {}
        stats = {"sections": {}, "total_input": len(items), "total_output": 0}

        def _process_section(section: str) -> Optional[tuple]:
            """å¤„ç†å•ä¸ª section çš„ç²¾æ’ + æ´å¯Ÿæå–ï¼Œè¿”å› (section, briefs, stat_dict) æˆ– None"""
            section_items = by_section[section]
            print(f"\n  ğŸ“ å¤„ç† '{section}': {len(section_items)} æ¡å€™é€‰")

            # é™åˆ¶æ¯ä¸ª section çš„è¾“å…¥é‡ï¼ˆé™ä½åˆ° 20 ä»¥åŠ é€Ÿï¼‰
            max_input = self.config.get("max_items_per_section", 20)
            if len(section_items) > max_input:
                section_items = sorted(section_items, key=lambda x: x.score, reverse=True)[:max_input]
                print(f"     ğŸ“Š æˆªå– Top {max_input}")

            # Step 1: ç²¾æ’
            ranked_ids = self._fine_rank(section_items, section)

            if ranked_ids:
                # æŒ‰ç²¾æ’ç»“æœé‡æ’
                id_to_item = {i: item for i, item in enumerate(section_items)}
                ranked_items = []
                for r in ranked_ids:
                    idx = r.get("id", -1)
                    if idx in id_to_item:
                        item = id_to_item[idx]
                        # é™„åŠ ç²¾æ’ä¿¡æ¯
                        item.metadata = item.metadata or {}
                        item.metadata["fine_rank"] = {
                            "relevance": r.get("relevance", 0),
                            "impact": r.get("impact", 0),
                            "urgency": r.get("urgency", 0),
                            "total": r.get("total", 0),
                            "priority": r.get("priority", "ğŸŸ¢"),
                        }
                        ranked_items.append(item)

                # å– top N
                ranked_items = ranked_items[:top_per_section]
                print(f"     âœ“ ç²¾æ’: {len(ranked_items)} æ¡é€šè¿‡")
            else:
                # ç²¾æ’å¤±è´¥ï¼Œfallback åˆ°ç²—æ’
                ranked_items = section_items[:top_per_section]
                print(f"     âš ï¸ ç²¾æ’å¤±è´¥ï¼Œä½¿ç”¨ç²—æ’ Top {top_per_section}")

            # Step 2: æ´å¯Ÿæå–
            if ranked_items:
                briefs = self._extract_insights(ranked_items, section)
                
                # æ³¨å…¥ç²¾æ’çš„ priorityï¼ˆå¦‚æœ AI æ²¡ç»™çš„è¯ï¼‰
                for i, brief in enumerate(briefs):
                    if i < len(ranked_items):
                        meta = ranked_items[i].metadata or {}
                        fr = meta.get("fine_rank", {})
                        if "priority" not in brief and fr.get("priority"):
                            brief["priority"] = fr["priority"]

                section_stat = {
                    "input": len(by_section[section]),
                    "after_fine_rank": len(ranked_items),
                    "output": len(briefs),
                }
                print(f"     âœ“ æ´å¯Ÿ: {len(briefs)} æ¡ briefs")
                return (section, briefs, section_stat)

            return None

        # å¹¶è¡Œå¤„ç†å„ sectionï¼ˆæ¯ section æœ€å¤š 180sï¼‰
        section_timeout = 180
        sections = sorted(by_section.keys())
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            future_to_section = {
                executor.submit(_process_section, section): section
                for section in sections
            }
            for future in as_completed(future_to_section):
                section = future_to_section[future]
                try:
                    result = future.result(timeout=section_timeout)
                    if result is not None:
                        sec_name, briefs, section_stat = result
                        all_briefs[sec_name] = briefs
                        stats["sections"][sec_name] = section_stat
                        stats["total_output"] += len(briefs)
                except Exception as e:
                    print(f"     âš ï¸ Section '{section}' å¤„ç†å¼‚å¸¸: {e}")

        # Step 3: Executive Summary
        executive_summary = ""
        if all_briefs:
            executive_summary = self._generate_executive_summary(all_briefs)

        print(f"\nâœ… AI åˆ†æå®Œæˆ: {stats['total_output']} æ¡ briefs")

        return {
            "briefs": all_briefs,
            "executive_summary": executive_summary,
            "stats": stats,
        }

    def _group_by_section(self, items: List[Item]) -> Dict[str, List[Item]]:
        """æŒ‰ section/channel åˆ†ç»„"""
        groups = defaultdict(list)
        for item in items:
            groups[item.channel].append(item)
        return dict(groups)

    def _fine_rank(self, items: List[Item], section: str) -> List[Dict]:
        """
        ç²¾æ’ï¼šClaude è¯„ä¼°æ¯æ¡ä»·å€¼
        
        Returns:
            æ’åºåçš„ [{id, relevance, impact, urgency, total, priority, reason}]
        """
        # åˆ†æ‰¹ï¼ˆé˜²è¶… tokenï¼‰
        batches = self.claude.batch_items_by_tokens(items, max_tokens=60000)
        all_ranked = []

        for batch_idx, batch in enumerate(batches):
            if len(batches) > 1:
                print(f"     ğŸ“¦ ç²¾æ’æ‰¹æ¬¡ {batch_idx+1}/{len(batches)}: {len(batch)} æ¡")

            prompt = self.prompts.fine_rank_prompt(batch, section)

            try:
                result = self.claude.call_with_json(
                    prompt=prompt,
                    system=self.prompts.system_prompt(),
                    max_tokens=4096,
                    temperature=0.2,
                )

                if isinstance(result, list):
                    # è°ƒæ•´ ID offsetï¼ˆå¤šæ‰¹æ¬¡æ—¶ï¼‰
                    offset = sum(len(batches[b]) for b in range(batch_idx)) if batch_idx > 0 else 0
                    for r in result:
                        r["id"] = r.get("id", 0) + offset
                    all_ranked.extend(result)

            except Exception as e:
                print(f"     âš ï¸ ç²¾æ’å¤±è´¥: {e}")

        # æŒ‰ total é™åº
        all_ranked.sort(key=lambda x: x.get("total", 0), reverse=True)
        return all_ranked

    def _extract_insights(self, items: List[Item], section: str) -> List[Dict]:
        """
        æ´å¯Ÿæå–ï¼šç”Ÿæˆ headline + detail + priority + tags
        """
        batches = self.claude.batch_items_by_tokens(items, max_tokens=60000)
        all_briefs = []

        for batch_idx, batch in enumerate(batches):
            if len(batches) > 1:
                print(f"     ğŸ“¦ æå–æ‰¹æ¬¡ {batch_idx+1}/{len(batches)}: {len(batch)} æ¡")

            prompt = self.prompts.insight_extract_prompt(batch, section)

            try:
                briefs = self.claude.call_with_json(
                    prompt=prompt,
                    system=self.prompts.system_prompt(),
                    max_tokens=16384,
                    temperature=0.3,
                )

                if isinstance(briefs, list):
                    all_briefs.extend(briefs)
                elif isinstance(briefs, dict) and "items" in briefs:
                    all_briefs.extend(briefs["items"])

            except Exception as e:
                print(f"     âš ï¸ æå–å¤±è´¥: {e}")
                # Fallback
                for item in batch:
                    meta = item.metadata or {}
                    all_briefs.append({
                        "headline": item.title,
                        "detail": item.text[:200],
                        "url": item.url,
                        "source": meta.get("feed_name") or item.source,
                        "priority": "ğŸŸ¢",
                        "tags": [],
                    })

        return all_briefs

    def _generate_executive_summary(self, all_briefs: Dict) -> str:
        """
        ç”Ÿæˆè·¨æ¿å— Executive Summary
        """
        from processors.generator import ReportGenerator
        # åŠ è½½ section configs
        section_configs_path = Path(__file__).parent.parent.parent / "config" / "sections.yaml"
        import yaml
        section_configs = {}
        if section_configs_path.exists():
            with open(section_configs_path) as f:
                data = yaml.safe_load(f)
                section_configs = data.get("sections", {})

        prompt = self.prompts.executive_summary_prompt(all_briefs, section_configs)

        try:
            summary = self.claude.call(
                prompt=prompt,
                system=self.prompts.system_prompt(),
                max_tokens=2048,
                temperature=0.3,
            )
            print(f"     âœ“ Executive Summary ç”Ÿæˆå®Œæˆ")
            return summary.strip()
        except Exception as e:
            print(f"     âš ï¸ Executive Summary ç”Ÿæˆå¤±è´¥: {e}")
            return ""

    def save_analyzed_data(self, result: Dict, output_path: Path):
        """ä¿å­˜åˆ†æç»“æœ"""
        output_path.parent.mkdir(parents=True, exist_ok=True)
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(result, f, indent=2, ensure_ascii=False)
        print(f"ğŸ’¾ å·²ä¿å­˜åˆ†ææ•°æ®: {output_path}")

    def load_analyzed_data(self, input_path: Path) -> Dict:
        """åŠ è½½åˆ†æç»“æœ"""
        with open(input_path, encoding="utf-8") as f:
            return json.load(f)
