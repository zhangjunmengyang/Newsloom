"""arXiv å­¦æœ¯è®ºæ–‡æ•°æ®æº"""

import httpx
from typing import List, Optional
from datetime import datetime, timezone, timedelta
from xml.etree import ElementTree as ET

from .base import DataSource, Item


class ArxivSource(DataSource):
    """
    arXiv å­¦æœ¯è®ºæ–‡æ•°æ®æº

    é…ç½®ç¤ºä¾‹:
    ```yaml
    arxiv:
      enabled: true
      channel: "papers"
      type: "arxiv"
      categories: "cat:cs.AI+OR+cat:cs.CL+OR+cat:cs.LG"
      max_results: 20
    ```
    """

    def get_source_name(self) -> str:
        return "arxiv"

    def fetch(self, hours_ago: Optional[int] = None) -> List[Item]:
        """æŠ“å– arXiv è®ºæ–‡"""
        categories = self.config.get('categories', 'cat:cs.AI')
        max_results = self.config.get('max_results', 20)

        # æ„å»ºæŸ¥è¯¢
        base_url = "http://export.arxiv.org/api/query"
        query = f"search_query={categories}&sortBy=submittedDate&sortOrder=descending&max_results={max_results}"
        url = f"{base_url}?{query}"

        print(f"    ğŸ”¬ æŠ“å– arXiv: {categories}")

        try:
            response = httpx.get(url, timeout=30, follow_redirects=True)
            response.raise_for_status()

            items = self._parse_feed(response.text, hours_ago)
            return items

        except Exception as e:
            print(f"    âš ï¸  arXiv æŠ“å–å¤±è´¥: {e}")
            return []

    def _parse_feed(self, xml_content: str, hours_ago: Optional[int]) -> List[Item]:
        """è§£æ arXiv Atom feed"""
        items = []
        cutoff_time = None

        if hours_ago is not None:
            cutoff_time = datetime.now(timezone.utc) - timedelta(hours=hours_ago)

        # è§£æ XML
        root = ET.fromstring(xml_content)

        # arXiv ä½¿ç”¨ Atom å‘½åç©ºé—´
        ns = {'atom': 'http://www.w3.org/2005/Atom'}

        for entry in root.findall('atom:entry', ns):
            try:
                # æå–å­—æ®µ
                title = entry.find('atom:title', ns).text.strip()
                title = ' '.join(title.split())  # æ¸…ç†æ¢è¡Œ

                summary = entry.find('atom:summary', ns).text.strip()
                summary = ' '.join(summary.split())

                # è®ºæ–‡ ID å’Œ URL
                arxiv_id = entry.find('atom:id', ns).text
                url = arxiv_id  # arXiv ID å°±æ˜¯ URL

                # PDF URL
                pdf_url = None
                for link in entry.findall('atom:link', ns):
                    if link.get('title') == 'pdf':
                        pdf_url = link.get('href')
                        break

                # ä½œè€…
                authors = []
                for author in entry.findall('atom:author', ns):
                    name = author.find('atom:name', ns)
                    if name is not None:
                        authors.append(name.text)

                author_str = ', '.join(authors[:3])  # åªå–å‰3ä¸ªä½œè€…
                if len(authors) > 3:
                    author_str += f' et al. ({len(authors)} authors)'

                # å‘å¸ƒæ—¶é—´
                published = entry.find('atom:published', ns).text
                published_at = datetime.fromisoformat(published.replace('Z', '+00:00'))

                # æ—¶æ•ˆè¿‡æ»¤
                if cutoff_time and published_at < cutoff_time:
                    continue

                # åˆ†ç±»
                categories = []
                for category in entry.findall('atom:category', ns):
                    cat_term = category.get('term')
                    if cat_term:
                        categories.append(cat_term)

                # åˆ›å»º metadata
                metadata = {
                    'feed_name': 'arXiv',
                    'authors': authors,
                    'categories': categories,
                    'pdf_url': pdf_url,
                    'arxiv_id': arxiv_id.split('/')[-1],  # æå–çº¯ ID
                }

                # åˆ›å»º Item
                item = self._make_item(
                    native_id=arxiv_id,
                    title=title,
                    text=summary,
                    url=url,
                    author=author_str,
                    published_at=published_at,
                    metadata=metadata
                )

                items.append(item)

            except Exception as e:
                print(f"    âš ï¸  è§£æ arXiv entry å¤±è´¥: {e}")
                continue

        return items
