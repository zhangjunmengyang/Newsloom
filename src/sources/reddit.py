"""Reddit æ•°æ®æº - é€šè¿‡å…¬å¼€ JSON endpoint æŠ“å–çƒ­é—¨å¸–å­"""

import httpx
from typing import List, Optional
from datetime import datetime, timezone, timedelta
from concurrent.futures import ThreadPoolExecutor, as_completed

from .base import DataSource, Item


class RedditSource(DataSource):
    """
    Reddit æ•°æ®æº

    ä½¿ç”¨å…¬å¼€ JSON endpoint (æ— éœ€ API key)
    æŠ“å–æŒ‡å®š subreddit çš„çƒ­é—¨å¸–å­

    é…ç½®ç¤ºä¾‹:
    ```yaml
    reddit:
      enabled: true
      channel: "community"
      type: "reddit"
      subreddits:
        - "MachineLearning"
        - "LocalLLaMA"
        - "CryptoCurrency"
        - "algotrading"
      sort: "hot"           # hot/top/new
      time_filter: "day"    # hour/day/week/month/year/all (ä»… sort=top æ—¶æœ‰æ•ˆ)
      limit: 10             # æ¯ä¸ª subreddit å–å¤šå°‘æ¡
      min_score: 50         # æœ€ä½Žåˆ†æ•°è¿‡æ»¤
    ```
    """

    BASE_URL = "https://www.reddit.com"

    def get_source_name(self) -> str:
        return "reddit"

    def fetch(self, hours_ago: Optional[int] = None) -> List[Item]:
        """æŠ“å– Reddit çƒ­é—¨å¸–å­"""
        subreddits = self.config.get('subreddits', ['MachineLearning'])
        sort = self.config.get('sort', 'hot')
        time_filter = self.config.get('time_filter', 'day')
        limit = self.config.get('limit', 10)
        min_score = self.config.get('min_score', 50)

        print(f"    ðŸ”´ æŠ“å– Reddit: {len(subreddits)} ä¸ª subreddit, sort={sort}")

        all_items = []

        # å¹¶å‘æŠ“å–å„ä¸ª subreddit
        with ThreadPoolExecutor(max_workers=min(len(subreddits), 5)) as executor:
            futures = {
                executor.submit(
                    self._fetch_subreddit, sub, sort, time_filter, limit, min_score, hours_ago
                ): sub
                for sub in subreddits
            }

            for future in as_completed(futures):
                sub = futures[future]
                try:
                    items = future.result()
                    all_items.extend(items)
                    print(f"      r/{sub}: {len(items)} æ¡")
                except Exception as e:
                    print(f"      âš ï¸  r/{sub} å¤±è´¥: {e}")

        print(f"    âœ… Reddit: èŽ·å–åˆ° {len(all_items)} æ¡å¸–å­")
        return all_items

    def _fetch_subreddit(
        self,
        subreddit: str,
        sort: str,
        time_filter: str,
        limit: int,
        min_score: int,
        hours_ago: Optional[int]
    ) -> List[Item]:
        """æŠ“å–å•ä¸ª subreddit"""
        url = f"{self.BASE_URL}/r/{subreddit}/{sort}.json"
        params = {
            'limit': limit * 2,  # å¤šå–ä¸€äº›ç”¨äºŽè¿‡æ»¤
            'raw_json': 1,
        }
        if sort == 'top':
            params['t'] = time_filter

        headers = {
            'User-Agent': 'Newsloom/0.2.0 (News Aggregator Bot)'
        }

        # ä»£ç†æ”¯æŒ
        proxy = self.config.get('proxy')
        client_kwargs = dict(timeout=30, follow_redirects=True)
        if proxy:
            client_kwargs['proxy'] = proxy

        response = httpx.get(url, params=params, headers=headers, **client_kwargs)
        response.raise_for_status()

        data = response.json()
        posts = data.get('data', {}).get('children', [])

        items = []
        cutoff_time = None
        if hours_ago:
            cutoff_time = datetime.now(timezone.utc) - timedelta(hours=hours_ago)

        for post_data in posts:
            if len(items) >= limit:
                break

            post = post_data.get('data', {})

            # è·³è¿‡ç½®é¡¶å’Œå¹¿å‘Š
            if post.get('stickied') or post.get('promoted'):
                continue

            # åˆ†æ•°è¿‡æ»¤
            score = post.get('score', 0)
            if score < min_score:
                continue

            # æ—¶é—´è¿‡æ»¤
            created_utc = post.get('created_utc', 0)
            published_at = datetime.fromtimestamp(created_utc, tz=timezone.utc)
            if cutoff_time and published_at < cutoff_time:
                continue

            # æå–å†…å®¹
            title = post.get('title', 'No title')
            selftext = post.get('selftext', '')
            post_url = post.get('url', '')
            permalink = post.get('permalink', '')
            reddit_url = f"https://www.reddit.com{permalink}" if permalink else post_url
            author = post.get('author', 'unknown')
            num_comments = post.get('num_comments', 0)
            upvote_ratio = post.get('upvote_ratio', 0)

            # æž„å»ºæ–‡æœ¬
            text_parts = []
            if selftext:
                # æˆªå–å‰ 500 å­—ç¬¦
                text_parts.append(selftext[:500])
            text_parts.append(f"\nâ¬†ï¸ {score} points | ðŸ’¬ {num_comments} comments | {upvote_ratio:.0%} upvoted")
            text = '\n'.join(text_parts)

            # metadata
            metadata = {
                'feed_name': f'r/{subreddit}',
                'subreddit': subreddit,
                'score': score,
                'num_comments': num_comments,
                'upvote_ratio': upvote_ratio,
                'reddit_url': reddit_url,
                'is_self': post.get('is_self', False),
            }

            # å¦‚æžœæ˜¯å¤–é“¾å¸–å­ï¼Œurl ç”¨å¤–é“¾ï¼›å¦åˆ™ç”¨ reddit é“¾æŽ¥
            link_url = post_url if not post.get('is_self') and post_url else reddit_url

            item = self._make_item(
                native_id=post.get('id', permalink),
                title=title,
                text=text,
                url=link_url,
                author=f"u/{author}",
                published_at=published_at,
                metadata=metadata
            )
            items.append(item)

        return items
